---
title: "LIME - Local Interpretable Model-agnostic Explanations"
output: html_document 
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Wprowadzenie do LIME

LIME (Local Interpretable Model-agnostic Explanations) to technika
tworzenia lokalnych wyjaśnień dla pojedynczych predykcji. Metoda działa
poprzez:

1.  Generowanie permutacji wokół badanej obserwacji
2.  Trenowanie prostego, interpretowalnego modelu na tych permutacjach
3.  Użycie tego modelu do wyjaśnienia lokalnego zachowania złożonego
    modelu

**Ważne ograniczenia:**

-   Niestabilność wyjaśnień przy małej liczbie próbek
-   Brak gwarancji globalnej spójności
-   Wrażliwość samej metody na parametry

**Jak radzić sobie z ograniczeniami?:**

-   **Parametry**: używaj 1000-5000 próbek dla stabilnych wyjaśnień,
    5-10 cech dla czytelności
-   **Walidacja**: zawsze sprawdzaj stabilność wyjaśnień, porównuj z
    wiedzą domenową
-   **Interpretacja**: uważaj na mylące korelacje, testuj na różnych
    obserwacjach

**Literatura**

-   Artykuł przedstawiający lime: <https://arxiv.org/pdf/1602.04938>
-   Krytyka lime i shap: <https://arxiv.org/pdf/1806.08049>

## Przygotowanie pakietów

```{r packages}
# Podstawowe biblioteki
library(randomForest)  # Model do wyjaśniania
library(dplyr)         # Manipulacja danych
library(lime)          # Implementacja LIME
library(ggplot2)       # Wizualizacja
library(reticulate)    # Integracja z Pythonem
```

## Przygotowanie danych

Połączymy preprocessing w python z R

```{python air_quality_download}
import pandas as pd

air_quality_df = pd.read_csv("../lab3/AirQualityUCI.csv", sep=";", decimal=",")

air_quality_df = air_quality_df.iloc[:, :-2]
air_quality_df['Date'] = pd.to_datetime(air_quality_df['Date'], format='%d/%m/%Y')
air_quality_df['Time'] = pd.to_datetime(air_quality_df['Time'], format='%H.%M.%S')

columns_rename = {
    'CO(GT)': 'CO',
    'PT08.S1(CO)': 'PT08_S1_CO',
    'NMHC(GT)': 'NMHC',
    'C6H6(GT)': 'Benzene',
    'PT08.S2(NMHC)': 'PT08_S2_NMHC',
    'NOx(GT)': 'NOx',
    'PT08.S3(NOx)': 'PT08_S3_NOx',
    'NO2(GT)': 'NO2',
    'PT08.S4(NO2)': 'PT08_S4_NO2',
    'PT08.S5(O3)': 'PT08_S5_O3',
    'T': 'Temperature',
    'RH': 'RelativeHumidity',
    'AH': 'AbsoluteHumidity'
}

air_quality_df = air_quality_df.rename(columns=columns_rename)

print(air_quality_df.info())
air_quality_df['NO2'].describe()
```

```{r data_preprocessing}
# Przekazujemy dane przez reticulate
air_data <- py$air_quality_df

# Czyszczenie danych
air_data <- air_data[, !sapply(air_data, function(x) all(is.na(x)))]
air_data <- air_data[, !names(air_data) %in% c("Date", "Time")]

# Konwersja wartości -200 (kodowanie pustych w tym zbiorze) na NA
air_data[air_data == -200] <- NA

# Usunięcie kolumn z więcej niż 50% brakujących danych
missing_summary <- sapply(air_data, function(x) sum(is.na(x)))
columns_to_keep <- names(missing_summary)[missing_summary <= nrow(air_data) * 0.5]
air_data <- air_data[, columns_to_keep]

# Imputacja medianą - bez tego stracimy ~90% danych
for(col in names(air_data)) {
  if(is.numeric(air_data[[col]])) {
    median_val <- median(air_data[[col]], na.rm = TRUE)
    air_data[[col]][is.na(air_data[[col]])] <- median_val
  }
}

# Tworzenie zmiennej docelowej i usunięcie NOx (zbyt skorelowany z NO2)
if("NO2" %in% names(air_data)) {
  air_data$high_pollution <- ifelse(air_data$NO2 > 140, 1, 0)
  air_clean <- air_data[, !names(air_data) %in% c("NO2", "NOx")]
} else if("CO" %in% names(air_data)) {
  air_data$high_pollution <- ifelse(air_data$CO > 8, 1, 0)
  air_clean <- air_data[, !names(air_data) %in% c("CO", "NOx")]
} else {
  numeric_cols <- sapply(air_data, is.numeric)
  target_col <- names(air_data)[numeric_cols][1]
  threshold <- quantile(air_data[[target_col]], 0.75, na.rm = TRUE)
  air_data$high_pollution <- ifelse(air_data[[target_col]] > threshold, 1, 0)
  air_clean <- air_data[, !names(air_data) %in% c(target_col, "NOx")]
}

# Dodatkowe usuwanie NOx jeśli nadal istnieje
if("NOx" %in% names(air_clean)) {
  air_clean <- air_clean[, !names(air_clean) %in% "NOx"]
}

air_clean$high_pollution <- as.factor(air_clean$high_pollution)

# Podział na zbiory
set.seed(123)
train_size <- floor(0.7 * nrow(air_clean))
train_indices <- sample(seq_len(nrow(air_clean)), size = train_size)
train_data <- air_clean[train_indices, ]
test_data <- air_clean[-train_indices, ]

cat("Rozkład zmiennej docelowej:\n")
print(table(air_clean$high_pollution))
cat("\nUsunieto NOx ze zbioru predyktorów\n")
cat("Pozostałe zmienne:", paste(names(air_clean)[names(air_clean) != "high_pollution"], collapse = ", "), "\n")
```

## Budowa modelu bazowego

```{r model_training}
# Trenowanie modelu Random Forest
rf_model <- randomForest(high_pollution ~ ., 
                        data = train_data, 
                        ntree = 500,
                        importance = TRUE)

# Ocena modelu
predictions <- predict(rf_model, test_data)
actual <- test_data$high_pollution

# Obliczenie dokładności
accuracy <- sum(predictions == actual) / length(actual)
cat("Dokładność modelu:", round(accuracy, 3), "\n")

# Macierz pomyłek
print("Macierz pomyłek:")
print(table(actual, predictions))
```

# Analiza LIME w R

## Przygotowanie explainera LIME

LIME domyślnie wspiera tylko określone typy modeli i raczej są to
rozbudowane narzędzia, których nie chcemy pobierać do demonstracji. Dla
RandomForest musimy zatem użyć funkcji as_classifier() lub
zaimplementować własną metodę predict_model().

```{r lime_preparation}
# Przygotowanie danych bez zmiennej docelowej
train_features <- train_data[, !names(train_data) %in% "high_pollution"]
test_features <- test_data[, !names(test_data) %in% "high_pollution"]

# Sprawdzenie nazw kolumn
cat("Nazwy cech w zbiorze treningowym:\n")
print(names(train_features))

# RandomForest nie jest wspierany domyślnie przez LIME
# Musimy zaimplementować metodę predict_model dla randomForest
predict_model.randomForest <- function(x, newdata, type, ...) {
  # Sprawdzenie czy to klasyfikacja czy regresja
  if(x$type == "classification") {
    # Dla klasyfikacji zwracamy prawdopodobieństwa
    res <- predict(x, newdata = newdata, type = "prob", ...)
    return(as.data.frame(res))
  } else {
    # Dla regresji zwracamy predykcje
    res <- predict(x, newdata = newdata, type = "response", ...)
    return(data.frame(Response = res))
  }
}

# Alternatywnie można użyć as_classifier() - prostsze podejście
rf_wrapped <- as_classifier(rf_model)

# Tworzenie explaineraa LIME z opakowaniem
lime_explainer <- lime(train_features, rf_wrapped, bin_continuous = TRUE)

cat("Explainer LIME został utworzony dla", ncol(train_features), "zmiennych\n")
```

## Generowanie wyjaśnień LIME

```{r lime_explanations}
# Wybór obserwacji do wyjaśnienia
set.seed(123)
explain_indices <- sample(1:nrow(test_features), 3)
observations_to_explain <- test_features[explain_indices, ]

# Generowanie wyjaśnień LIME z opakowaniem modelu
lime_explanations <- explain(
  observations_to_explain, 
  lime_explainer, 
  n_labels = 1,      # liczba klas do wyjaśnienia
  n_features = 6,    # liczba najważniejszych cech
  n_permutations = 1000  # zwiększona liczba permutacji dla stabilności
)

# Podstawowy wykres wyjaśnień
plot_features(lime_explanations)

# Alternatywny wykres
plot_explanations(lime_explanations)
```

**1. Przeanalizuj wyjaśnienia LIME dla pierwszej obserwacji. Które
czynniki środowiskowe najbardziej wpływają na predykcję? Zinterpretuj
kierunki wpływu?**

# LIME w Pythonie

## Przygotowanie danych i modelu w Pythonie

```{python lime_python_setup}
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import lime
import lime.lime_tabular

# Przygotowanie danych (wykorzystujemy dane z R)
air_data_py = r.air_clean.copy()

# Usunięcie NOx jeśli nadal istnieje
if 'NOx' in air_data_py.columns:
    air_data_py = air_data_py.drop('NOx', axis=1)
    print("Usunięto NOx ze zbioru danych Python")

# Przygotowanie features i target
X = air_data_py.drop('high_pollution', axis=1)
y = air_data_py['high_pollution'].astype(int)

print(f"Zmienne w zbiorze Python: {list(X.columns)}")
print(f"Sprawdzenie czy NOx został usunięty: {'NOx' not in X.columns}")

# Podział na zbiory
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=123, stratify=y
)

print(f"Kształt danych treningowych: {X_train.shape}")
print(f"Kształt danych testowych: {X_test.shape}")
```

## Trenowanie modelu w Pythonie

```{python python_model}
# Trenowanie modelu Random Forest
rf_python = RandomForestClassifier(n_estimators=500, random_state=123)
rf_python.fit(X_train.values, y_train.values)

# Ocena modelu
y_pred = rf_python.predict(X_test.values)
accuracy = accuracy_score(y_test, y_pred)

print(f"Dokładność modelu (Python): {accuracy:.3f}")
print("\nRaport klasyfikacji:")
print(classification_report(y_test, y_pred))

# Ważność cech
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf_python.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 5 najważniejszych cech:")
print(feature_importance.head())
```

## LIME w Pythonie - podstawowe zastosowanie

```{python lime_python_basic}
# Tworzenie explainera LIME z poprawnymi nazwami cech
explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train.values,
    feature_names=list(X_train.columns),
    class_names=['Low', 'High'],
    mode='classification',
    discretize_continuous=True
)

# Wybór obserwacji do wyjaśnienia. Można podmienić, żeby przeanalizować kilka różnych
instance_idx = 0
instance = X_test.iloc[instance_idx].values

print(f"Wyjaśnianie obserwacji {instance_idx}:")
print(f"Rzeczywista etykieta: {y_test.iloc[instance_idx]}")
print(f"Predykcja modelu: {rf_python.predict([instance])[0]}")
print(f"Prawdopodobieństwa: {rf_python.predict_proba([instance])[0]}")

# Generowanie wyjaśnienia z większą liczbą próbek
explanation = explainer.explain_instance(
    instance, 
    rf_python.predict_proba, 
    num_features=6,
    num_samples=1000  # Zwiększona liczba próbek
)

# Wyświetlenie wyjaśnienia w formie tekstowej
print("\nWyjaśnienie LIME:")
for feature, weight in explanation.as_list():
    print(f"{feature}: {weight:.4f}")
```

## Wizualizacja LIME w Pythonie

```{python lime_python_viz}
# Zapisanie wykresu jako HTML (można wyświetlić w przeglądarce)
explanation.save_to_file('lime_explanation.html')

# Wyświetlenie wyjaśnienia jako mapa wartości
explanation_map = explanation.as_map()[1]  # klasa 1 (high pollution)
sorted_explanation = sorted(explanation_map, key=lambda x: abs(x[1]), reverse=True)

print("Mapa wyjaśnień (posortowana według ważności):")
feature_names = list(X_train.columns)
for feature_idx, weight in sorted_explanation:
    feature_name = feature_names[feature_idx]
    print(f"{feature_name}: {weight:.4f}")
```

## Analiza wielu obserwacji w Pythonie

```{python lime_python_multiple}
# Funkcja pomocnicza do analizy wielu przypadków
def analyze_multiple_instances(explainer, X_test, y_test, rf_model, n_instances=3):
    results = []
    
    for i in range(min(n_instances, len(X_test))):
        instance = X_test.iloc[i].values
        actual = y_test.iloc[i]
        predicted = rf_model.predict([instance])[0]
        prob = rf_model.predict_proba([instance])[0]
        
        # Generowanie wyjaśnienia
        explanation = explainer.explain_instance(
            instance, 
            rf_model.predict_proba, 
            num_features=5,
            num_samples=1000
        )
        
        results.append({
            'instance_id': i,
            'actual': actual,
            'predicted': predicted,
            'probability': prob[1],  # prawdopodobieństwo klasy pozytywnej
            'explanation': explanation.as_list()
        })
        
        print(f"\n--- Instancja {i} ---")
        print(f"Rzeczywista: {actual}, Predykcja: {predicted}, Prob: {prob[1]:.3f}")
        print("Top 3 cechy:")
        for feature, weight in explanation.as_list()[:3]:
            print(f"  {feature}: {weight:.4f}")
    
    return results

# Analiza 3 przypadków
analysis_results = analyze_multiple_instances(explainer, X_test, y_test, rf_python, 3)
```

**2. Porównaj wyjaśnienia LIME otrzymane w R i Pythonie dla podobnych
obserwacji. Czy są różnice w interpretacji? Co może być ich przyczyną?**

# Zaawansowane funkcjonalności LIME

## Analiza stabilności wyjaśnień

```{r lime_stability}
# Test stabilności - wiele wyjaśnień dla tej samej obserwacji
test_instance <- observations_to_explain[1, ]

# Generowanie kilku wyjaśnień z różną liczbą próbek
stability_test <- list()
sample_sizes <- c(500, 1000, 2000)

for(i in seq_along(sample_sizes)) {
  explanation <- explain(
    test_instance, 
    lime_explainer, 
    n_labels = 1,
    n_features = 6,
    n_permutations = sample_sizes[i]
  )
  
  stability_test[[i]] <- explanation[, c("feature", "feature_weight")]
  names(stability_test)[i] <- paste("samples", sample_sizes[i], sep = "_")
}

# Porównanie stabilności
for(i in seq_along(stability_test)) {
  print(stability_test[[i]])
}
```

## Identyfikacja przypadków granicznych

```{r edge_cases}
# Znajdowanie obserwacji z niepewną predykcją (blisko 0.5)
predict_proba <- function(model, newdata) {
  predict(model, newdata, type = "prob")[, "1"]
}

all_predictions <- predict_proba(rf_model, test_features)
uncertain_indices <- which(abs(all_predictions - 0.5) < 0.15)

cat("Liczba niepewnych predykcji:", length(uncertain_indices), "\n")

if(length(uncertain_indices) > 0) {
  # Analiza niepewnego przypadku
  uncertain_case <- test_features[uncertain_indices[1], ]
  
  cat("Niepewny przypadek - predykcja:", 
      round(all_predictions[uncertain_indices[1]], 3), "\n")
  
  # LIME dla niepewnego przypadku - używamy opakowanego modelu
  uncertain_lime <- explain(
    uncertain_case, 
    lime_explainer, 
    n_labels = 1,
    n_features = 8,
    n_permutations = 1500
  )
  
  print(uncertain_lime[, c("feature", "feature_weight", "feature_desc")])
  
  plot_features(uncertain_lime)
}
```

## Porównanie z ważnością globalną

```{r global_vs_local}
# Globalna ważność z Random Forest
rf_importance <- importance(rf_model)[, "MeanDecreaseGini"]
rf_importance_df <- data.frame(
  variable = names(rf_importance),
  global_importance = as.numeric(rf_importance)
) %>% arrange(desc(global_importance))

# Średnia lokalna ważność z LIME (dla przeanalizowanych przypadków)
lime_summary <- lime_explanations %>%
  group_by(feature) %>%
  summarise(
    avg_weight = mean(abs(feature_weight)),
    appearances = n(),
    .groups = 'drop'
  ) %>%
  arrange(desc(avg_weight))

# Globalna ważność (Random Forest):
print(head(rf_importance_df, 5))

# Średnia lokalna ważność (LIME):
print(head(lime_summary, 5))
```

**3. Wybierz kilka przypadków o różnych poziomach pewności predykcji
(wysoka pewność pozytywna, wysoka pewność negatywna, niepewne przypadki)
i porównaj ich wyjaśnienia LIME. Jakie wzorce można zaobserwować?**
